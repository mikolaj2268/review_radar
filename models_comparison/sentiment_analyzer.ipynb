{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik Parquet zosta≈Ç pomy≈õlnie wczytany.\n",
      "Dostƒôpne kolumny: ['review_id', 'at', 'content', 'score', 'app_name']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ≈öcie≈ºka do pliku Parquet\n",
    "parquet_path = 'models_comparison/reviews.parquet'\n",
    "\n",
    "# Sprawdzenie, czy plik Parquet istnieje\n",
    "if not os.path.exists(parquet_path):\n",
    "    print(f\"Plik Parquet nie zosta≈Ç znaleziony pod ≈õcie≈ºkƒÖ: {parquet_path}\")\n",
    "    exit()\n",
    "\n",
    "# Wczytaj dane z pliku Parquet\n",
    "try:\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(\"Plik Parquet zosta≈Ç pomy≈õlnie wczytany.\")\n",
    "except Exception as e:\n",
    "    print(f\"WystƒÖpi≈Ç b≈ÇƒÖd podczas wczytywania pliku Parquet: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Sprawdzenie, czy kolumna 'content' istnieje\n",
    "print(\"Dostƒôpne kolumny:\", df.columns.tolist())\n",
    "if 'content' not in df.columns:\n",
    "    print(\"Kolumna 'content' nie zosta≈Ça znaleziona w pliku Parquet.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content\n",
      "<class 'str'>    698688\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['content'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## obliczenie sentymentu dla 4 modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is enabled\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check and set the device\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS is enabled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zwracanie wszystkich wartosci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (Metal Performance Shaders) device is available. Using GPU acceleration.\n",
      "Parquet file successfully loaded.\n",
      "Available columns: ['review_id', 'at', 'content', 'score', 'app_name']\n",
      "Loading DistilBERT model...\n",
      "Loading Cardiff NLP RoBERTa model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment using TextBlob...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 698688/698688 [00:44<00:00, 15646.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment using VADER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 698688/698688 [00:16<00:00, 42523.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment using DistilBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing DistilBERT in Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21834/21834 [30:15<00:00, 12.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment using Cardiff NLP RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing RoBERTa in Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21834/21834 [1:05:41<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              review_id          at  \\\n",
      "0  4423e3f0-6002-469c-bf91-239a1ba1d998  2024-11-23   \n",
      "1  16d24c6a-d3ea-4558-9a6b-2694beb581ec  2024-11-23   \n",
      "2  14098fe6-6649-4fb4-aeb1-f1b36fdcca6f  2024-11-23   \n",
      "3  6b215132-0eaf-40f7-a7f4-758c8fbf35b1  2024-11-23   \n",
      "4  b57b2878-92ba-4468-a3dc-722f2c61b51b  2024-11-23   \n",
      "\n",
      "                                             content  score app_name  \\\n",
      "0                                      No comment üò≠üòî      5  Netflix   \n",
      "1  lately the sounds go thru but no picture...its...      2  Netflix   \n",
      "2                                 This is very goodüëç      5  Netflix   \n",
      "3  Why auto payment with bank is activated after ...      1  Netflix   \n",
      "4  Keeps updating every 2 days and suddenly canno...      1  Netflix   \n",
      "\n",
      "   textblob_pattern_polarity  textblob_pattern_subjectivity  vader_neg  \\\n",
      "0                       0.00                           0.00      0.500   \n",
      "1                      -0.55                           0.75      0.205   \n",
      "2                       0.20                           0.30      0.000   \n",
      "3                       0.00                           0.00      0.000   \n",
      "4                       0.00                           0.50      0.000   \n",
      "\n",
      "   vader_neu  vader_pos  vader_compound  distilbert_negative_prob  \\\n",
      "0      0.377      0.123         -0.6124                  0.987631   \n",
      "1      0.584      0.211          0.0225                  0.999301   \n",
      "2      0.610      0.390          0.4927                  0.034944   \n",
      "3      1.000      0.000          0.0000                  0.996918   \n",
      "4      1.000      0.000          0.0000                  0.999739   \n",
      "\n",
      "   distilbert_positive_prob  distilbert_sentiment_score  \\\n",
      "0                  0.012369                   -0.975263   \n",
      "1                  0.000699                   -0.998603   \n",
      "2                  0.965056                    0.930112   \n",
      "3                  0.003082                   -0.993837   \n",
      "4                  0.000261                   -0.999478   \n",
      "\n",
      "  distilbert_sentiment_label  roberta_negative_prob  roberta_neutral_prob  \\\n",
      "0                   NEGATIVE               0.817729              0.172929   \n",
      "1                   NEGATIVE               0.913306              0.082218   \n",
      "2                   POSITIVE               0.004413              0.011624   \n",
      "3                   NEGATIVE               0.207172              0.769924   \n",
      "4                   NEGATIVE               0.779235              0.208691   \n",
      "\n",
      "   roberta_positive_prob roberta_sentiment_label  \n",
      "0               0.009343                NEGATIVE  \n",
      "1               0.004477                NEGATIVE  \n",
      "2               0.983962                POSITIVE  \n",
      "3               0.022904                 NEUTRAL  \n",
      "4               0.012074                NEGATIVE  \n",
      "Results have been saved to: models_comparison/reviews_with_sentiments.parquet\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Set Up Device for MacBook M2\n",
    "# ----------------------------\n",
    "\n",
    "# Check and set the device to MPS (Metal Performance Shaders) if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS (Metal Performance Shaders) device is available. Using GPU acceleration.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU device found. Using CPU.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load and Validate Data\n",
    "# ----------------------------\n",
    "\n",
    "# Path to the Parquet file\n",
    "parquet_path = 'models_comparison/reviews.parquet'\n",
    "\n",
    "# Check if the Parquet file exists\n",
    "if not os.path.exists(parquet_path):\n",
    "    print(f\"Parquet file not found at path: {parquet_path}\")\n",
    "    exit()\n",
    "\n",
    "# Load data from the Parquet file\n",
    "try:\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(\"Parquet file successfully loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Parquet file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Check if the 'content' column exists\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "if 'content' not in df.columns:\n",
    "    print(\"The 'content' column was not found in the Parquet file.\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Initialize Sentiment Analyzers\n",
    "# ----------------------------\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Initialize DistilBERT tokenizer and model\n",
    "print(\"Loading DistilBERT model...\")\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "distilbert_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "distilbert_model.to(device)\n",
    "distilbert_model.eval()\n",
    "\n",
    "# Initialize Cardiff NLP RoBERTa model\n",
    "print(\"Loading Cardiff NLP RoBERTa model...\")\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "roberta_model.to(device)\n",
    "roberta_model.eval()\n",
    "\n",
    "# Initialize tqdm pandas extension\n",
    "tqdm.pandas()\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Define Sentiment Analysis Functions\n",
    "# ----------------------------\n",
    "\n",
    "# Function to analyze sentiment using TextBlob with PatternAnalyzer only\n",
    "def analyze_textblob(text):\n",
    "    try:\n",
    "        blob = TextBlob(str(text))\n",
    "        sentiment = blob.sentiment  # returns Sentiment(polarity, subjectivity)\n",
    "\n",
    "        return {\n",
    "            'textblob_pattern_polarity': sentiment.polarity,\n",
    "            'textblob_pattern_subjectivity': sentiment.subjectivity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"TextBlob error for text: {text}\\nError: {e}\")\n",
    "        return {\n",
    "            'textblob_pattern_polarity': None,\n",
    "            'textblob_pattern_subjectivity': None\n",
    "        }\n",
    "\n",
    "# Function to analyze sentiment using VADER\n",
    "def analyze_vader(text):\n",
    "    try:\n",
    "        scores = analyzer.polarity_scores(str(text))\n",
    "        return {\n",
    "            'vader_neg': scores['neg'],\n",
    "            'vader_neu': scores['neu'],\n",
    "            'vader_pos': scores['pos'],\n",
    "            'vader_compound': scores['compound']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"VADER error for text: {text}\\nError: {e}\")\n",
    "        return {\n",
    "            'vader_neg': None,\n",
    "            'vader_neu': None,\n",
    "            'vader_pos': None,\n",
    "            'vader_compound': None\n",
    "        }\n",
    "\n",
    "# Function to analyze sentiment using DistilBERT in batches\n",
    "def analyze_distilbert_batch(texts, batch_size=32):\n",
    "    sentiments = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing DistilBERT in Batches\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        try:\n",
    "            inputs = distilbert_tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = distilbert_model(**inputs)\n",
    "\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            probs = probabilities.tolist()\n",
    "\n",
    "            for prob in probs:\n",
    "                negative_prob, positive_prob = prob\n",
    "                sentiment_score = positive_prob - negative_prob\n",
    "\n",
    "                if positive_prob > negative_prob:\n",
    "                    sentiment_label = \"POSITIVE\"\n",
    "                elif positive_prob < negative_prob:\n",
    "                    sentiment_label = \"NEGATIVE\"\n",
    "                else:\n",
    "                    sentiment_label = \"NEUTRAL\"\n",
    "\n",
    "                sentiments.append({\n",
    "                    'distilbert_negative_prob': negative_prob,\n",
    "                    'distilbert_positive_prob': positive_prob,\n",
    "                    'distilbert_sentiment_score': sentiment_score,\n",
    "                    'distilbert_sentiment_label': sentiment_label\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"DistilBERT error for batch {i}-{i+batch_size}: {e}\")\n",
    "            for _ in batch:\n",
    "                sentiments.append({\n",
    "                    'distilbert_negative_prob': None,\n",
    "                    'distilbert_positive_prob': None,\n",
    "                    'distilbert_sentiment_score': None,\n",
    "                    'distilbert_sentiment_label': None\n",
    "                })\n",
    "    return sentiments\n",
    "\n",
    "# Function to analyze sentiment using Cardiff NLP RoBERTa in batches\n",
    "def analyze_roberta_batch(texts, batch_size=32):\n",
    "    sentiments = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing RoBERTa in Batches\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        try:\n",
    "            inputs = roberta_tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = roberta_model(**inputs)\n",
    "\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            probs = probabilities.tolist()\n",
    "\n",
    "            for prob in probs:\n",
    "                negative_prob, neutral_prob, positive_prob = prob\n",
    "                sentiment_label = max(\n",
    "                    [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"],\n",
    "                    key=lambda x: {\"NEGATIVE\": negative_prob, \"NEUTRAL\": neutral_prob, \"POSITIVE\": positive_prob}[x],\n",
    "                )\n",
    "                sentiments.append({\n",
    "                    'roberta_negative_prob': negative_prob,\n",
    "                    'roberta_neutral_prob': neutral_prob,\n",
    "                    'roberta_positive_prob': positive_prob,\n",
    "                    'roberta_sentiment_label': sentiment_label\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"RoBERTa error for batch {i}-{i+batch_size}: {e}\")\n",
    "            for _ in batch:\n",
    "                sentiments.append({\n",
    "                    'roberta_negative_prob': None,\n",
    "                    'roberta_neutral_prob': None,\n",
    "                    'roberta_positive_prob': None,\n",
    "                    'roberta_sentiment_label': None\n",
    "                })\n",
    "    return sentiments\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Apply Sentiment Analyses\n",
    "# ----------------------------\n",
    "\n",
    "# Apply TextBlob sentiment analysis with progress bar\n",
    "print(\"Analyzing sentiment using TextBlob...\")\n",
    "textblob_results = df['content'].progress_apply(analyze_textblob).tolist()\n",
    "textblob_df = pd.DataFrame(textblob_results)\n",
    "df = pd.concat([df.reset_index(drop=True), textblob_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Apply VADER sentiment analysis with progress bar\n",
    "print(\"Analyzing sentiment using VADER...\")\n",
    "vader_results = df['content'].progress_apply(analyze_vader).tolist()\n",
    "vader_df = pd.DataFrame(vader_results)\n",
    "df = pd.concat([df.reset_index(drop=True), vader_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Apply DistilBERT sentiment analysis in batches with progress bar\n",
    "print(\"Analyzing sentiment using DistilBERT...\")\n",
    "distilbert_results = analyze_distilbert_batch(df['content'].tolist())\n",
    "distilbert_df = pd.DataFrame(distilbert_results)\n",
    "df = pd.concat([df.reset_index(drop=True), distilbert_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Apply Cardiff NLP RoBERTa sentiment analysis in batches with progress bar\n",
    "print(\"Analyzing sentiment using Cardiff NLP RoBERTa...\")\n",
    "roberta_results = analyze_roberta_batch(df['content'].tolist())\n",
    "roberta_df = pd.DataFrame(roberta_results)\n",
    "df = pd.concat([df.reset_index(drop=True), roberta_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# 6. View and Save Results\n",
    "# ----------------------------\n",
    "\n",
    "# Display the first few rows of the DataFrame with sentiment scores\n",
    "print(df.head())\n",
    "\n",
    "# Optionally, save the results to a new Parquet file\n",
    "output_path = 'models_comparison/reviews_with_sentiments.parquet'\n",
    "try:\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    print(f\"Results have been saved to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Parquet file:{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zwracanie pojedynczych wartosci sentymentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (Metal Performance Shaders) device is available. Using GPU acceleration.\n",
      "Parquet file successfully loaded.\n",
      "Available columns: ['review_id', 'at', 'content', 'score', 'app_name']\n",
      "Loading DistilBERT model...\n",
      "Analyzing sentiment using TextBlob...\n",
      "Analyzing sentiment using VADER...\n",
      "Analyzing sentiment using DistilBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing DistilBERT in Batches:  37%|‚ñà‚ñà‚ñà‚ñã      | 7989/21834 [10:31<18:15, 12.64it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 163\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Apply DistilBERT sentiment analysis in batches\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing sentiment using DistilBERT...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_distilbert\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_distilbert_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# 6. View and Save Results\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the DataFrame with sentiment scores\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[41], line 130\u001b[0m, in \u001b[0;36manalyze_distilbert_batch\u001b[0;34m(texts, batch_size)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Apply softmax to get probabilities\u001b[39;00m\n\u001b[1;32m    129\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mprobabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prob \u001b[38;5;129;01min\u001b[39;00m probs:\n\u001b[1;32m    133\u001b[0m     negative_prob, positive_prob \u001b[38;5;241m=\u001b[39m prob\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Set Up Device for MacBook M2\n",
    "# ----------------------------\n",
    "\n",
    "# Check and set the device to MPS (Metal Performance Shaders) if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS (Metal Performance Shaders) device is available. Using GPU acceleration.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU device found. Using CPU.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load and Validate Data\n",
    "# ----------------------------\n",
    "\n",
    "# Path to the Parquet file\n",
    "parquet_path = 'models_comparison/reviews.parquet'\n",
    "\n",
    "# Check if the Parquet file exists\n",
    "if not os.path.exists(parquet_path):\n",
    "    print(f\"Parquet file not found at path: {parquet_path}\")\n",
    "    exit()\n",
    "\n",
    "# Load data from the Parquet file\n",
    "try:\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(\"Parquet file successfully loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Parquet file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Check if the 'content' column exists\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "if 'content' not in df.columns:\n",
    "    print(\"The 'content' column was not found in the Parquet file.\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Initialize Sentiment Analyzers\n",
    "# ----------------------------\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Initialize DistilBERT tokenizer and model\n",
    "print(\"Loading DistilBERT model...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model.to(device)  # Move the model to the selected device (MPS/GPU/CPU)\n",
    "model.eval()      # Set the model to evaluation mode\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Define Sentiment Analysis Functions\n",
    "# ----------------------------\n",
    "\n",
    "# Function to analyze sentiment using TextBlob\n",
    "def analyze_textblob(text):\n",
    "    try:\n",
    "        blob = TextBlob(str(text))\n",
    "        return blob.sentiment.polarity\n",
    "    except Exception as e:\n",
    "        print(f\"TextBlob error for text: {text}\\nError: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to analyze sentiment using VADER\n",
    "def analyze_vader(text):\n",
    "    try:\n",
    "        scores = analyzer.polarity_scores(str(text))\n",
    "        return scores['compound']\n",
    "    except Exception as e:\n",
    "        print(f\"VADER error for text: {text}\\nError: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to analyze sentiment using DistilBERT (calculating sentiment score by subtracting negative probability from positive probability)\n",
    "def analyze_distilbert(text):\n",
    "    try:\n",
    "        # Tokenize the input text and move tensors to the selected device\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Extract negative and positive probabilities\n",
    "        negative_prob, positive_prob = probabilities[0].tolist()\n",
    "        \n",
    "        # Calculate sentiment score\n",
    "        sentiment_score = positive_prob - negative_prob\n",
    "        \n",
    "        # Determine sentiment label based on the sentiment score\n",
    "        if sentiment_score > 0:\n",
    "            sentiment_label = \"POSITIVE\"\n",
    "        elif sentiment_score < 0:\n",
    "            sentiment_label = \"NEGATIVE\"\n",
    "        else:\n",
    "            sentiment_label = \"NEUTRAL\"\n",
    "        \n",
    "        return sentiment_score\n",
    "    except Exception as e:\n",
    "        print(f\"DistilBERT error for text: {text}\\nError: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to analyze sentiment using DistilBERT in batches\n",
    "def analyze_distilbert_batch(texts, batch_size=32):\n",
    "    sentiments = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing DistilBERT in Batches\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        try:\n",
    "            # Tokenize the batch and move tensors to the selected device\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            probs = probabilities.tolist()\n",
    "            \n",
    "            for prob in probs:\n",
    "                negative_prob, positive_prob = prob\n",
    "                sentiment_score = positive_prob - negative_prob\n",
    "                \n",
    "                if sentiment_score > 0:\n",
    "                    sentiment_label = \"POSITIVE\"\n",
    "                elif sentiment_score < 0:\n",
    "                    sentiment_label = \"NEGATIVE\"\n",
    "                else:\n",
    "                    sentiment_label = \"NEUTRAL\"\n",
    "                \n",
    "                sentiments.append(sentiment_score)\n",
    "        except Exception as e:\n",
    "            print(f\"DistilBERT error for batch {i}-{i+batch_size}: {e}\")\n",
    "            sentiments.extend([None]*len(batch))\n",
    "    return sentiments\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Apply Sentiment Analyses\n",
    "# ----------------------------\n",
    "\n",
    "# Apply TextBlob sentiment analysis\n",
    "print(\"Analyzing sentiment using TextBlob...\")\n",
    "df['sentiment_textblob'] = df['content'].apply(analyze_textblob)\n",
    "\n",
    "# Apply VADER sentiment analysis\n",
    "print(\"Analyzing sentiment using VADER...\")\n",
    "df['sentiment_vader'] = df['content'].apply(analyze_vader)\n",
    "\n",
    "# Apply DistilBERT sentiment analysis in batches\n",
    "print(\"Analyzing sentiment using DistilBERT...\")\n",
    "df['sentiment_distilbert'] = analyze_distilbert_batch(df['content'].tolist())\n",
    "\n",
    "# ----------------------------\n",
    "# 6. View and Save Results\n",
    "# ----------------------------\n",
    "\n",
    "# Display the first few rows of the DataFrame with sentiment scores\n",
    "print(df.head())\n",
    "\n",
    "# Optionally, save the results to a new Parquet file\n",
    "output_path = 'models_comparison/reviews_with_sentiments.parquet'\n",
    "try:\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    print(f\"Results have been saved to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Parquet file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pierwsze podejscie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: this app is alright but not great\n",
      "Sentiment: NEGATIVE\n",
      "Score: -0.9938241243362427\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis using DistilBERT.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: Sentiment label and score.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get logits and apply softmax to compute probabilities\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Extract probabilities and determine label\n",
    "    negative_prob, positive_prob = probabilities[0].tolist()\n",
    "    sentiment_label = \"POSITIVE\" if positive_prob > negative_prob else \"NEGATIVE\"\n",
    "    sentiment_score = positive_prob if sentiment_label == \"POSITIVE\" else -negative_prob\n",
    "\n",
    "    return {\n",
    "        \"label\": sentiment_label,\n",
    "        \"score\": sentiment_score,\n",
    "    }\n",
    "\n",
    "text = \"this app is alright but not great\"\n",
    "result = analyze_sentiment(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Sentiment: {result['label']}\")\n",
    "print(f\"Score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drugie podejscie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: this app is alright but not great\n",
      "Sentiment: NEGATIVE\n",
      "Score: -0.9876482994295657\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis using DistilBERT with a subtraction approach.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: Sentiment label and score.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get logits and apply softmax to compute probabilities\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Extract probabilities for negative and positive sentiments\n",
    "    negative_prob, positive_prob = probabilities[0].tolist()\n",
    "\n",
    "    # Calculate sentiment score by subtracting negative probability from positive probability\n",
    "    sentiment_score = positive_prob - negative_prob\n",
    "\n",
    "    # Determine sentiment label based on the sentiment score\n",
    "    if sentiment_score > 0:\n",
    "        sentiment_label = \"POSITIVE\"\n",
    "    elif sentiment_score < 0:\n",
    "        sentiment_label = \"NEGATIVE\"\n",
    "    else:\n",
    "        sentiment_label = \"NEUTRAL\"\n",
    "\n",
    "    return {\n",
    "        \"label\": sentiment_label,\n",
    "        \"score\": sentiment_score,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "text = \"this app is alright but not great\"\n",
    "result = analyze_sentiment(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Sentiment: {result['label']}\")\n",
    "print(f\"Score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
